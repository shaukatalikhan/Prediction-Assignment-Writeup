---
title: "Prediction Assignment Writeup"
author: "Shaukat Ali"
date: "Sunday, March 22, 2015"
output: html_document
---
##Synopsis
In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset).

The goal of our project is to predict the manner in which they did the exercise.

## Data Sources

The training data for this project are available here: 

[https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

The test data are available here: 

[https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

The data for this project comes from this original source: [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har). 

##Downloading the data

```{r cache = TRUE}
url<-"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(url,destfile="./data/pml-training.csv")
url1<-"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url1,destfile="./data/pml-testing.csv")
```

##Reading the data

```{r cache=TRUE}
training<-read.csv("./data/pml-training.csv", na.strings=c("NA","#DIV/0!",""))
testing<-read.csv("./data/pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
```           


## Libraries Used and simulation setting

The following Libraries were used for this project.
```{r}
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
```

Finally, load the same seed with the following line of code:
```{r}
set.seed(12345)
```

##Partioning the training set into two

Partioning Training data set into two data sets, 60% for myTraining, 40% for myTesting:

```{r}
Train <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
myTraining <- training[Train, ]
myTesting <- training[-Train, ]
dim(myTraining); dim(myTesting)
```


## Cleaning the data

The following methods were used to clean the data:

Change 1: Cleaning NearZeroVariance Variables
Run this code to view possible NZV Variables:
```{r}
myDataNZV <- nearZeroVar(myTraining, saveMetrics=TRUE)
```

Run this code to create another subset without NZV variables:

```{r}
myNearZeroVar <- names(myTraining) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt",
"kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt",
"max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm",
"var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
"stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm",
"kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
"max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
"kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell",
"skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell",
"amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
"skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm",
"max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
"amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm",
"avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm",
"stddev_yaw_forearm", "var_yaw_forearm")
myTraining <- myTraining[!myNearZeroVar]
#To check the new N?? of observations
dim(myTraining)
```

Transformation 2: Killing first column of Dataset - ID
Removing first ID variable so that it does not interfer with ML Algorithms:

```{r}
myTraining <- myTraining[c(-1)]
```

Transformation 3: Cleaning Variables with too many NAs.
We will  leae out those variables that have more than a 60% threshold of NA's.

```{r}

training_new <- myTraining 
for(i in 1:length(myTraining)) { 
        if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .6 ) { #if n?? NAs > 60% of total observations
  	for(j in 1:length(training_new)) {
			if( length( grep(names(myTraining[i]), names(training_new)[j]) ) ==1)  { 
        
				training_new <- training_new[ , -j] #Remove that column
			}	
		} 
	}
}

dim(training_new)
myTraining <- training_new
rm(training_new)
```

Now let us do the exact same 3 transformations but for our myTesting and testing data sets.

```{r}
new1 <- colnames(myTraining)
new2 <- colnames(myTraining[, -58])
myTesting <- myTesting[new1]
testing <- testing[new2]
dim(myTesting)
dim(testing)
```

In order to ensure proper functioning of Decision Trees and especially RandomForest Algorithm with the Test data set (data set provided), we need to coerce the data into the same type.

```{r}
for (i in 1:length(testing) ) {
        for(j in 1:length(myTraining)) {
		if( length( grep(names(myTraining[i]), names(testing)[j]) ) ==1)  {
			class(testing[j]) <- class(myTraining[i])
		}      
	}      
}

testing <- rbind(myTraining[2, -58] , testing)  
testing <- testing[-1,]
```

## Using ML algorithms for prediction: Decision Tree

```{r}
fit1 <- rpart(classe ~ ., data=myTraining, method="class")
```

Note: to view the decision tree with fancy run this command:

```{r}
fancyRpartPlot(fit1)
```

Predicting:

```{r}
predictions1 <- predict(fit1, myTesting, type = "class")
```

(Moment of truth) Using confusion Matrix to test results:
```{r}
confusionMatrix(predictions1, myTesting$classe)
#Overall Statistics
                                          
#               Accuracy : 0.8683          
#                 95% CI : (0.8607, 0.8757)
#    No Information Rate : 0.2845          
#    P-Value [Acc > NIR] : < 2.2e-16       
                                          
#                  Kappa : 0.8335 
```

## Using ML algorithms for prediction: Random Forests

```{r}
fit2 <- randomForest(classe ~. , data=myTraining)
```

Predicting:
```{r}
predictions2 <- predict(fit2, myTesting, type = "class")
```
(Moment of truth) Using confusion Matrix to test results:
```{r}
confusionMatrix(predictions2, myTesting$classe)
#Overall Statistics
                                         
 #              Accuracy : 0.999          
 #                95% CI : (0.998, 0.9996)
 #   No Information Rate : 0.2845         
 #   P-Value [Acc > NIR] : < 2.2e-16      
                                         
 #                 Kappa : 0.9987         
 #Mcnemar's Test P-Value : NA 


```
Random Forests yielded better Results, as expected!

## Generating Files to submit as answers for the Assignment:

Finally, using the provided Test Set:
Note:
#For Decision Tree would be like this, but not going to use it:
#predictionsA2 <- predict(fit1, testing, type = "class")

For Random Forests is, which yielded a much better prediction:

```{r}
predictionsB2 <- predict(fit2, testing, type = "class")
```

The following function produces files with prdictions which will be submitted for assignment.

```{r}

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(predictionsB2)
```